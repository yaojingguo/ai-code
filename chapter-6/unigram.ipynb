{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb946a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.064626"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.006802 - 0.071428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f4028a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.064626"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.071428 - 0.006802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b34c611d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07823000000000001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.006802 + 0.071428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92ada09",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0271f93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0713ac16932f45e48f66e58e5882e128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/397 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433c74c344a248dc963cf57c69a382e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01d4e3b6eef44f4af3504b3109e4472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5490305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'▁This': 3,\n",
       "             '▁is': 2,\n",
       "             '▁the': 1,\n",
       "             '▁Hugging': 1,\n",
       "             '▁Face': 1,\n",
       "             '▁Course.': 1,\n",
       "             '▁chapter': 1,\n",
       "             '▁about': 1,\n",
       "             '▁tokenization.': 1,\n",
       "             '▁section': 1,\n",
       "             '▁shows': 1,\n",
       "             '▁several': 1,\n",
       "             '▁tokenizer': 1,\n",
       "             '▁algorithms.': 1,\n",
       "             '▁Hopefully,': 1,\n",
       "             '▁you': 1,\n",
       "             '▁will': 1,\n",
       "             '▁be': 1,\n",
       "             '▁able': 1,\n",
       "             '▁to': 1,\n",
       "             '▁understand': 1,\n",
       "             '▁how': 1,\n",
       "             '▁they': 1,\n",
       "             '▁are': 1,\n",
       "             '▁trained': 1,\n",
       "             '▁and': 1,\n",
       "             '▁generate': 1,\n",
       "             '▁tokens.': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b56311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁t', 7),\n",
       " ('is', 5),\n",
       " ('er', 5),\n",
       " ('▁a', 5),\n",
       " ('▁to', 4),\n",
       " ('to', 4),\n",
       " ('en', 4),\n",
       " ('▁T', 3),\n",
       " ('▁Th', 3),\n",
       " ('▁Thi', 3)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        # Loop through the subwords of length at least 2\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq\n",
    "\n",
    "# Sort subwords by frequency\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_subwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6136b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token, freq in token_freqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aafad8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁': 31,\n",
       " 'T': 3,\n",
       " 'h': 9,\n",
       " 'i': 13,\n",
       " 's': 13,\n",
       " 't': 14,\n",
       " 'e': 21,\n",
       " 'H': 2,\n",
       " 'u': 6,\n",
       " 'g': 5,\n",
       " 'n': 11,\n",
       " 'F': 1,\n",
       " 'a': 12,\n",
       " 'c': 3,\n",
       " 'C': 1,\n",
       " 'o': 13,\n",
       " 'r': 9,\n",
       " '.': 4,\n",
       " 'p': 2,\n",
       " 'b': 3,\n",
       " 'k': 3,\n",
       " 'z': 2,\n",
       " 'w': 3,\n",
       " 'v': 1,\n",
       " 'l': 7,\n",
       " 'm': 1,\n",
       " 'f': 1,\n",
       " 'y': 3,\n",
       " ',': 1,\n",
       " 'd': 4,\n",
       " '▁t': 7,\n",
       " 'is': 5,\n",
       " 'er': 5,\n",
       " '▁a': 5,\n",
       " '▁to': 4,\n",
       " 'to': 4,\n",
       " 'en': 4,\n",
       " '▁T': 3,\n",
       " '▁Th': 3,\n",
       " '▁Thi': 3,\n",
       " '▁This': 3,\n",
       " 'Th': 3,\n",
       " 'Thi': 3,\n",
       " 'This': 3,\n",
       " 'hi': 3,\n",
       " 'his': 3,\n",
       " 'th': 3,\n",
       " 'ou': 3,\n",
       " 'se': 3,\n",
       " '▁tok': 3,\n",
       " '▁toke': 3,\n",
       " '▁token': 3,\n",
       " 'tok': 3,\n",
       " 'toke': 3,\n",
       " 'token': 3,\n",
       " 'ok': 3,\n",
       " 'oke': 3,\n",
       " 'oken': 3,\n",
       " 'ke': 3,\n",
       " 'ken': 3,\n",
       " '▁s': 3,\n",
       " 'ra': 3,\n",
       " 'nd': 3,\n",
       " '▁i': 2,\n",
       " '▁is': 2,\n",
       " '▁th': 2,\n",
       " '▁the': 2,\n",
       " 'the': 2,\n",
       " 'he': 2,\n",
       " '▁H': 2,\n",
       " 'in': 2,\n",
       " 'rs': 2,\n",
       " 'te': 2,\n",
       " '▁ab': 2,\n",
       " 'ab': 2,\n",
       " '▁tokeni': 2,\n",
       " '▁tokeniz': 2,\n",
       " 'tokeni': 2,\n",
       " 'tokeniz': 2,\n",
       " 'okeni': 2,\n",
       " 'okeniz': 2,\n",
       " 'keni': 2,\n",
       " 'keniz': 2,\n",
       " 'eni': 2,\n",
       " 'eniz': 2,\n",
       " 'ni': 2,\n",
       " 'niz': 2,\n",
       " 'iz': 2,\n",
       " 'at': 2,\n",
       " 'ti': 2,\n",
       " 'tio': 2,\n",
       " 'tion': 2,\n",
       " 'io': 2,\n",
       " 'ion': 2,\n",
       " 'on': 2,\n",
       " '▁se': 2,\n",
       " 'ho': 2,\n",
       " 'how': 2,\n",
       " 'ow': 2,\n",
       " 'era': 2,\n",
       " 'al': 2,\n",
       " 's.': 2,\n",
       " 'll': 2,\n",
       " 'an': 2,\n",
       " 'and': 2,\n",
       " 'ne': 2,\n",
       " '▁Hu': 1,\n",
       " '▁Hug': 1,\n",
       " '▁Hugg': 1,\n",
       " '▁Huggi': 1,\n",
       " '▁Huggin': 1,\n",
       " '▁Hugging': 1,\n",
       " 'Hu': 1,\n",
       " 'Hug': 1,\n",
       " 'Hugg': 1,\n",
       " 'Huggi': 1,\n",
       " 'Huggin': 1,\n",
       " 'Hugging': 1,\n",
       " 'ug': 1,\n",
       " 'ugg': 1,\n",
       " 'uggi': 1,\n",
       " 'uggin': 1,\n",
       " 'ugging': 1,\n",
       " 'gg': 1,\n",
       " 'ggi': 1,\n",
       " 'ggin': 1,\n",
       " 'gging': 1,\n",
       " 'gi': 1,\n",
       " 'gin': 1,\n",
       " 'ging': 1,\n",
       " 'ing': 1,\n",
       " 'ng': 1,\n",
       " '▁F': 1,\n",
       " '▁Fa': 1,\n",
       " '▁Fac': 1,\n",
       " '▁Face': 1,\n",
       " 'Fa': 1,\n",
       " 'Fac': 1,\n",
       " 'Face': 1,\n",
       " 'ac': 1,\n",
       " 'ace': 1,\n",
       " 'ce': 1,\n",
       " '▁C': 1,\n",
       " '▁Co': 1,\n",
       " '▁Cou': 1,\n",
       " '▁Cour': 1,\n",
       " '▁Cours': 1,\n",
       " '▁Course': 1,\n",
       " '▁Course.': 1,\n",
       " 'Co': 1,\n",
       " 'Cou': 1,\n",
       " 'Cour': 1,\n",
       " 'Cours': 1,\n",
       " 'Course': 1,\n",
       " 'Course.': 1,\n",
       " 'our': 1,\n",
       " 'ours': 1,\n",
       " 'ourse': 1,\n",
       " 'ourse.': 1,\n",
       " 'ur': 1,\n",
       " 'urs': 1,\n",
       " 'urse': 1,\n",
       " 'urse.': 1,\n",
       " 'rse': 1,\n",
       " 'rse.': 1,\n",
       " 'se.': 1,\n",
       " 'e.': 1,\n",
       " '▁c': 1,\n",
       " '▁ch': 1,\n",
       " '▁cha': 1,\n",
       " '▁chap': 1,\n",
       " '▁chapt': 1,\n",
       " '▁chapte': 1,\n",
       " '▁chapter': 1,\n",
       " 'ch': 1,\n",
       " 'cha': 1,\n",
       " 'chap': 1,\n",
       " 'chapt': 1,\n",
       " 'chapte': 1,\n",
       " 'chapter': 1,\n",
       " 'ha': 1,\n",
       " 'hap': 1,\n",
       " 'hapt': 1,\n",
       " 'hapte': 1,\n",
       " 'hapter': 1,\n",
       " 'ap': 1,\n",
       " 'apt': 1,\n",
       " 'apte': 1,\n",
       " 'apter': 1,\n",
       " 'pt': 1,\n",
       " 'pte': 1,\n",
       " 'pter': 1,\n",
       " 'ter': 1,\n",
       " '▁abo': 1,\n",
       " '▁abou': 1,\n",
       " '▁about': 1,\n",
       " 'abo': 1,\n",
       " 'abou': 1,\n",
       " 'about': 1,\n",
       " 'bo': 1,\n",
       " 'bou': 1,\n",
       " 'bout': 1,\n",
       " 'out': 1,\n",
       " 'ut': 1,\n",
       " '▁tokeniza': 1,\n",
       " '▁tokenizat': 1,\n",
       " '▁tokenizati': 1,\n",
       " '▁tokenizatio': 1,\n",
       " '▁tokenization': 1,\n",
       " '▁tokenization.': 1,\n",
       " 'tokeniza': 1,\n",
       " 'tokenizat': 1,\n",
       " 'tokenizati': 1,\n",
       " 'tokenizatio': 1,\n",
       " 'tokenization': 1,\n",
       " 'tokenization.': 1,\n",
       " 'okeniza': 1,\n",
       " 'okenizat': 1,\n",
       " 'okenizati': 1,\n",
       " 'okenizatio': 1,\n",
       " 'okenization': 1,\n",
       " 'okenization.': 1,\n",
       " 'keniza': 1,\n",
       " 'kenizat': 1,\n",
       " 'kenizati': 1,\n",
       " 'kenizatio': 1,\n",
       " 'kenization': 1,\n",
       " 'kenization.': 1,\n",
       " 'eniza': 1,\n",
       " 'enizat': 1,\n",
       " 'enizati': 1,\n",
       " 'enizatio': 1,\n",
       " 'enization': 1,\n",
       " 'enization.': 1,\n",
       " 'niza': 1,\n",
       " 'nizat': 1,\n",
       " 'nizati': 1,\n",
       " 'nizatio': 1,\n",
       " 'nization': 1,\n",
       " 'nization.': 1,\n",
       " 'iza': 1,\n",
       " 'izat': 1,\n",
       " 'izati': 1,\n",
       " 'izatio': 1,\n",
       " 'ization': 1,\n",
       " 'ization.': 1,\n",
       " 'za': 1,\n",
       " 'zat': 1,\n",
       " 'zati': 1,\n",
       " 'zatio': 1,\n",
       " 'zation': 1,\n",
       " 'zation.': 1,\n",
       " 'ati': 1,\n",
       " 'atio': 1,\n",
       " 'ation': 1,\n",
       " 'ation.': 1,\n",
       " 'tion.': 1,\n",
       " 'ion.': 1,\n",
       " 'on.': 1,\n",
       " 'n.': 1,\n",
       " '▁sec': 1,\n",
       " '▁sect': 1,\n",
       " '▁secti': 1,\n",
       " '▁sectio': 1,\n",
       " '▁section': 1,\n",
       " 'sec': 1,\n",
       " 'sect': 1,\n",
       " 'secti': 1,\n",
       " 'sectio': 1,\n",
       " 'section': 1,\n",
       " 'ec': 1,\n",
       " 'ect': 1,\n",
       " 'ecti': 1,\n",
       " 'ectio': 1,\n",
       " 'ection': 1,\n",
       " 'ct': 1,\n",
       " 'cti': 1,\n",
       " 'ctio': 1,\n",
       " 'ction': 1,\n",
       " '▁sh': 1,\n",
       " '▁sho': 1,\n",
       " '▁show': 1,\n",
       " '▁shows': 1,\n",
       " 'sh': 1,\n",
       " 'sho': 1,\n",
       " 'show': 1,\n",
       " 'shows': 1,\n",
       " 'hows': 1,\n",
       " 'ows': 1,\n",
       " 'ws': 1,\n",
       " '▁sev': 1,\n",
       " '▁seve': 1,\n",
       " '▁sever': 1,\n",
       " '▁severa': 1,\n",
       " '▁several': 1,\n",
       " 'sev': 1,\n",
       " 'seve': 1,\n",
       " 'sever': 1,\n",
       " 'severa': 1,\n",
       " 'several': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7036b185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('name', 'xiaoyu'), ('age', 10)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"name\": \"xiaoyu\", \"age\": 10}.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2427e058",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_items' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxiaoyu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m}\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_items' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "{\"name\": \"xiaoyu\", \"age\": 10}.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75269f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29ff9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    for start_idx in range(len(word)):\n",
    "        # This should be properly filled by the previous steps of the loop\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                # If we have found a better segmentation ending at end_idx, we update\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # We did not find a tokenization of the word -> unknown\n",
    "        return [\"<unk>\"], None\n",
    "\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3705941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n",
      "(['This'], 6.288267030694535)\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hopefully\", model))\n",
    "print(encode_word(\"This\", model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc3e77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f21a6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413.10377642940875"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a681c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # We always keep tokens of length 1\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbe2b1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.376412403623874\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "scores = compute_scores(model)\n",
    "print(scores[\"ll\"])\n",
    "print(scores[\"his\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6c4a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # Remove percent_to_remove tokens with the lowest scores.\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49ef905a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁This',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁Hugging',\n",
       " '▁Face',\n",
       " '▁',\n",
       " 'c',\n",
       " 'ou',\n",
       " 'r',\n",
       " 's',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "tokenize(\"This is the Hugging Face course.\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7d299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ff24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b65913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a060f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

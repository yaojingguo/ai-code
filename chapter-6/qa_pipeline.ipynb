{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ed78ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://hf-mirror.com/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9802603125572205,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back ðŸ¤— Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494bceff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9714872241020203,\n",
       " 'start': 1892,\n",
       " 'end': 1919,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context = \"\"\"\n",
    "ðŸ¤— Transformers: State of the Art NLP\n",
    "\n",
    "ðŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ðŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question_answerer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ccf4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72bf479413045668a1bb9ebc45f02b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42ae10a33be4c41940987dd2e940b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35eacb6d06945a6a09d5e31ca365e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7963eccec965447ba5a0efe0523d4468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f3805fea694323a2b8e1aad9fd7264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fea6fa86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which deep learning libraries back ðŸ¤— Transformers?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a25953cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\\nbetween them. It's straightforward to train your models with one before loading them for inference with the other.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f83062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BatchEncoding.sequence_ids of {'input_ids': tensor([[  101,  5979,  1996,  3776,  9818,  1171,   100, 25267,   136,   102,\n",
       "           100, 25267,  1110,  5534,  1118,  1103,  1210,  1211,  1927,  1996,\n",
       "          3776,  9818,   783, 13612,   117,   153,  1183,  1942,  1766,  1732,\n",
       "           117,  1105,  5157, 21484,  2271,  6737,   783,  1114,   170,  2343,\n",
       "          1306,  2008,  9111,  1206,  1172,   119,  1135,   112,   188, 21546,\n",
       "          1106,  2669,  1240,  3584,  1114,  1141,  1196, 10745,  1172,  1111,\n",
       "          1107, 16792,  1114,  1103,  1168,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.sequence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f5edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a929bdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67]) torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f07329a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5979,  1996,  3776,  9818,  1171,   100, 25267,   136,   102,\n",
       "           100, 25267,  1110,  5534,  1118,  1103,  1210,  1211,  1927,  1996,\n",
       "          3776,  9818,   783, 13612,   117,   153,  1183,  1942,  1766,  1732,\n",
       "           117,  1105,  5157, 21484,  2271,  6737,   783,  1114,   170,  2343,\n",
       "          1306,  2008,  9111,  1206,  1172,   119,  1135,   112,   188, 21546,\n",
       "          1106,  2669,  1240,  3584,  1114,  1141,  1196, 10745,  1172,  1111,\n",
       "          1107, 16792,  1114,  1103,  1168,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "713f2ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9a2f003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67]) torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d313bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None]\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d4624c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f816867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21eace5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.4531e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1185e-06, 1.3470e-05,\n",
       "        2.4368e-07, 2.1236e-06, 1.3220e-06, 3.7722e-04, 6.9219e-03, 1.0237e-05,\n",
       "        4.3289e-06, 1.5143e-05, 3.2463e-07, 4.1933e-06, 1.6808e-04, 9.9179e-01,\n",
       "        8.6288e-06, 3.8557e-04, 5.9956e-06, 4.3725e-06, 5.8977e-07, 3.0929e-06,\n",
       "        3.8998e-06, 2.9493e-06, 2.1939e-04, 5.4713e-06, 7.1354e-06, 2.3212e-05,\n",
       "        5.2711e-06, 4.7788e-07, 2.4291e-07, 4.4467e-07, 1.4879e-08, 4.8133e-08,\n",
       "        3.7169e-07, 7.1242e-08, 3.1735e-07, 2.2365e-07, 1.3685e-06, 2.4093e-08,\n",
       "        1.1470e-08, 4.4891e-07, 2.2828e-08, 5.2562e-07, 5.8093e-07, 1.6419e-06,\n",
       "        1.4114e-08, 2.0591e-07, 2.0161e-08, 2.5390e-07, 2.3251e-08, 1.4667e-08,\n",
       "        5.4533e-08, 2.4235e-08, 5.5391e-09, 1.8524e-08, 3.6818e-08, 3.4721e-08,\n",
       "        0.0000e+00], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "832d1a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = start_probabilities[:, None] * end_probabilities[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25b86d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([67]), torch.Size([67]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_probabilities.shape, end_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c7a7e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([67, 1]), torch.Size([1, 67]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_probabilities[:, None].shape, end_probabilities[None, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a02d29e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.triu(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f7574dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.4340e-13, 0.0000e+00, 0.0000e+00,  ..., 1.1023e-12, 1.6345e-12,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.1136e-14, 1.3514e-13,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.2744e-13,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]], grad_fn=<TriuBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33ae1de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9803, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "print(scores[start_index, end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c0bfe67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1576"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2834bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "779f2f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jax, PyTorch, and TensorFlow'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fc96052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Jax, PyTorch, and TensorFlow', 'start': 78, 'end': 106, 'score': tensor(0.9803, grad_fn=<SelectBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9e686fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, long_context)\n",
    "print(len(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "879c97cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internal [SEP]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46742cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This sentence is not [SEP]\n",
      "[CLS] is not too long [SEP]\n",
      "[CLS] too long but we [SEP]\n",
      "[CLS] but we are going [SEP]\n",
      "[CLS] are going to split [SEP]\n",
      "[CLS] to split it anyway [SEP]\n",
      "[CLS] it anyway. [SEP]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
    "inputs = tokenizer(\n",
    "    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f664cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "236858c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84d12edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"This sentence is not too long but we are going to split it anyway.\",\n",
    "    \"This sentence is shorter but will still get split.\",\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")\n",
    "\n",
    "print(inputs[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b30ee02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16796737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "inputs = inputs.convert_to_tensors(\"pt\")\n",
    "print(inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be5d4278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384]) torch.Size([2, 384])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "204f23b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3213374/431288513.py:7: UserWarning: An output with one or more elements was resized since it had shape [1, 384], which does not match the required output shape [2, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "# Mask all the [PAD] tokens\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c74b78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de1eca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 18, 0.3386708199977875), (173, 184, 0.9714869856834412)]\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "89f2d306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4a8ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '\\nðŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.3386708199977875}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714869856834412}\n"
     ]
    }
   ],
   "source": [
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc2cdcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(offsets),len(offsets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2151e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc20f2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates), len(candidates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c9b0577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0),\n",
       "  (0, 5),\n",
       "  (6, 10),\n",
       "  (11, 19),\n",
       "  (20, 29),\n",
       "  (30, 34),\n",
       "  (35, 36),\n",
       "  (37, 49),\n",
       "  (49, 50),\n",
       "  (0, 0),\n",
       "  (1, 2),\n",
       "  (3, 15),\n",
       "  (15, 16),\n",
       "  (17, 22),\n",
       "  (23, 25),\n",
       "  (26, 29),\n",
       "  (30, 33),\n",
       "  (34, 36),\n",
       "  (36, 37),\n",
       "  (39, 40),\n",
       "  (41, 53),\n",
       "  (54, 62),\n",
       "  (63, 72),\n",
       "  (73, 75),\n",
       "  (76, 79),\n",
       "  (79, 82),\n",
       "  (82, 86),\n",
       "  (87, 93),\n",
       "  (94, 96),\n",
       "  (97, 104),\n",
       "  (105, 110),\n",
       "  (111, 113),\n",
       "  (114, 119),\n",
       "  (120, 124),\n",
       "  (125, 127),\n",
       "  (128, 142),\n",
       "  (142, 143),\n",
       "  (144, 155),\n",
       "  (156, 166),\n",
       "  (166, 167),\n",
       "  (168, 176),\n",
       "  (177, 186),\n",
       "  (186, 187),\n",
       "  (188, 191),\n",
       "  (191, 194),\n",
       "  (194, 201),\n",
       "  (201, 202),\n",
       "  (203, 214),\n",
       "  (214, 215),\n",
       "  (216, 220),\n",
       "  (221, 231),\n",
       "  (232, 235),\n",
       "  (236, 240),\n",
       "  (241, 243),\n",
       "  (244, 248),\n",
       "  (249, 252),\n",
       "  (253, 262),\n",
       "  (262, 263),\n",
       "  (264, 267),\n",
       "  (268, 271),\n",
       "  (272, 274),\n",
       "  (275, 277),\n",
       "  (278, 282),\n",
       "  (283, 290),\n",
       "  (290, 291),\n",
       "  (291, 295),\n",
       "  (296, 298),\n",
       "  (298, 299),\n",
       "  (300, 306),\n",
       "  (307, 309),\n",
       "  (310, 313),\n",
       "  (314, 317),\n",
       "  (318, 326),\n",
       "  (326, 327),\n",
       "  (329, 330),\n",
       "  (331, 343),\n",
       "  (344, 352),\n",
       "  (353, 356),\n",
       "  (356, 357),\n",
       "  (358, 360),\n",
       "  (361, 368),\n",
       "  (369, 377),\n",
       "  (378, 381),\n",
       "  (382, 385),\n",
       "  (386, 391),\n",
       "  (392, 395),\n",
       "  (395, 398),\n",
       "  (398, 402),\n",
       "  (403, 409),\n",
       "  (410, 412),\n",
       "  (413, 414),\n",
       "  (415, 420),\n",
       "  (421, 425),\n",
       "  (425, 426),\n",
       "  (427, 431),\n",
       "  (431, 432),\n",
       "  (432, 436),\n",
       "  (437, 441),\n",
       "  (442, 444),\n",
       "  (445, 449),\n",
       "  (450, 453),\n",
       "  (454, 458),\n",
       "  (458, 462),\n",
       "  (463, 466),\n",
       "  (467, 471),\n",
       "  (472, 477),\n",
       "  (478, 482),\n",
       "  (483, 487),\n",
       "  (488, 491),\n",
       "  (492, 501),\n",
       "  (502, 504),\n",
       "  (505, 508),\n",
       "  (509, 514),\n",
       "  (515, 518),\n",
       "  (518, 519),\n",
       "  (520, 522),\n",
       "  (523, 526),\n",
       "  (527, 531),\n",
       "  (532, 536),\n",
       "  (536, 537),\n",
       "  (538, 542),\n",
       "  (543, 544),\n",
       "  (544, 546),\n",
       "  (546, 549),\n",
       "  (550, 556),\n",
       "  (557, 565),\n",
       "  (566, 568),\n",
       "  (569, 581),\n",
       "  (582, 584),\n",
       "  (585, 590),\n",
       "  (591, 596),\n",
       "  (596, 599),\n",
       "  (599, 601),\n",
       "  (602, 605),\n",
       "  (606, 609),\n",
       "  (610, 612),\n",
       "  (613, 621),\n",
       "  (622, 624),\n",
       "  (625, 631),\n",
       "  (632, 637),\n",
       "  (638, 646),\n",
       "  (647, 658),\n",
       "  (658, 659),\n",
       "  (661, 664),\n",
       "  (665, 671),\n",
       "  (672, 673),\n",
       "  (674, 677),\n",
       "  (678, 687),\n",
       "  (687, 690),\n",
       "  (690, 691),\n",
       "  (693, 694),\n",
       "  (694, 695),\n",
       "  (696, 700),\n",
       "  (700, 701),\n",
       "  (701, 703),\n",
       "  (703, 704),\n",
       "  (704, 707),\n",
       "  (708, 713),\n",
       "  (713, 714),\n",
       "  (714, 716),\n",
       "  (716, 717),\n",
       "  (717, 720),\n",
       "  (720, 721),\n",
       "  (721, 724),\n",
       "  (725, 731),\n",
       "  (731, 732),\n",
       "  (735, 736),\n",
       "  (737, 741),\n",
       "  (742, 753),\n",
       "  (754, 756),\n",
       "  (757, 759),\n",
       "  (759, 760),\n",
       "  (761, 764),\n",
       "  (765, 767),\n",
       "  (767, 768),\n",
       "  (769, 774),\n",
       "  (774, 775),\n",
       "  (778, 779),\n",
       "  (780, 783),\n",
       "  (784, 791),\n",
       "  (792, 794),\n",
       "  (795, 800),\n",
       "  (801, 804),\n",
       "  (805, 814),\n",
       "  (815, 818),\n",
       "  (819, 832),\n",
       "  (832, 833),\n",
       "  (836, 837),\n",
       "  (838, 841),\n",
       "  (842, 846),\n",
       "  (846, 847),\n",
       "  (847, 853),\n",
       "  (854, 862),\n",
       "  (862, 866),\n",
       "  (867, 871),\n",
       "  (872, 876),\n",
       "  (877, 882),\n",
       "  (883, 890),\n",
       "  (891, 893),\n",
       "  (894, 899),\n",
       "  (899, 900),\n",
       "  (903, 904),\n",
       "  (905, 906),\n",
       "  (907, 914),\n",
       "  (915, 918),\n",
       "  (919, 922),\n",
       "  (923, 928),\n",
       "  (929, 932),\n",
       "  (933, 936),\n",
       "  (937, 940),\n",
       "  (940, 943),\n",
       "  (943, 947),\n",
       "  (948, 954),\n",
       "  (954, 955),\n",
       "  (958, 959),\n",
       "  (960, 965),\n",
       "  (966, 969),\n",
       "  (969, 973),\n",
       "  (974, 979),\n",
       "  (979, 980),\n",
       "  (981, 988),\n",
       "  (989, 995),\n",
       "  (996, 1000),\n",
       "  (1000, 1005),\n",
       "  (1005, 1006),\n",
       "  (1008, 1009),\n",
       "  (1009, 1010),\n",
       "  (1011, 1022),\n",
       "  (1023, 1026),\n",
       "  (1027, 1032),\n",
       "  (1033, 1040),\n",
       "  (1041, 1047),\n",
       "  (1048, 1055),\n",
       "  (1056, 1058),\n",
       "  (1059, 1065),\n",
       "  (1066, 1068),\n",
       "  (1068, 1071),\n",
       "  (1071, 1076),\n",
       "  (1076, 1077),\n",
       "  (1080, 1081),\n",
       "  (1082, 1083),\n",
       "  (1083, 1086),\n",
       "  (1086, 1088),\n",
       "  (1088, 1092),\n",
       "  (1092, 1095),\n",
       "  (1096, 1099),\n",
       "  (1100, 1106),\n",
       "  (1107, 1110),\n",
       "  (1110, 1114),\n",
       "  (1115, 1119),\n",
       "  (1120, 1123),\n",
       "  (1124, 1134),\n",
       "  (1135, 1140),\n",
       "  (1140, 1141),\n",
       "  (1144, 1145),\n",
       "  (1146, 1148),\n",
       "  (1148, 1151),\n",
       "  (1151, 1152),\n",
       "  (1153, 1155),\n",
       "  (1156, 1168),\n",
       "  (1168, 1169),\n",
       "  (1170, 1174),\n",
       "  (1175, 1179),\n",
       "  (1180, 1182),\n",
       "  (1182, 1183),\n",
       "  (1183, 1186),\n",
       "  (1187, 1190),\n",
       "  (1190, 1193),\n",
       "  (1193, 1197),\n",
       "  (1198, 1204),\n",
       "  (1204, 1205),\n",
       "  (1206, 1210),\n",
       "  (1211, 1213),\n",
       "  (1214, 1218),\n",
       "  (1219, 1223),\n",
       "  (1224, 1227),\n",
       "  (1228, 1237),\n",
       "  (1237, 1238),\n",
       "  (1240, 1241),\n",
       "  (1241, 1242),\n",
       "  (1243, 1246),\n",
       "  (1246, 1249),\n",
       "  (1250, 1253),\n",
       "  (1254, 1259),\n",
       "  (1260, 1269),\n",
       "  (1270, 1273),\n",
       "  (1274, 1279),\n",
       "  (1280, 1284),\n",
       "  (1285, 1287),\n",
       "  (1288, 1289),\n",
       "  (1290, 1295),\n",
       "  (1295, 1296),\n",
       "  (1296, 1297),\n",
       "  (1298, 1306),\n",
       "  (1306, 1307),\n",
       "  (1310, 1311),\n",
       "  (1312, 1317),\n",
       "  (1318, 1323),\n",
       "  (1323, 1324),\n",
       "  (1324, 1326),\n",
       "  (1326, 1327),\n",
       "  (1327, 1330),\n",
       "  (1330, 1331),\n",
       "  (1331, 1334),\n",
       "  (1335, 1341),\n",
       "  (1342, 1344),\n",
       "  (1345, 1346),\n",
       "  (1347, 1352),\n",
       "  (1353, 1355),\n",
       "  (1356, 1360),\n",
       "  (1360, 1361),\n",
       "  (1364, 1365),\n",
       "  (1366, 1370),\n",
       "  (1371, 1372),\n",
       "  (1373, 1379),\n",
       "  (1380, 1385),\n",
       "  (1386, 1393),\n",
       "  (1394, 1395),\n",
       "  (1395, 1396),\n",
       "  (1396, 1397),\n",
       "  (1397, 1398),\n",
       "  (1398, 1399),\n",
       "  (1399, 1400),\n",
       "  (1400, 1401),\n",
       "  (1401, 1402),\n",
       "  (1402, 1403),\n",
       "  (1403, 1405),\n",
       "  (1405, 1407),\n",
       "  (1408, 1417),\n",
       "  (1417, 1418),\n",
       "  (1419, 1421),\n",
       "  (1422, 1426),\n",
       "  (1426, 1427),\n",
       "  (1430, 1431),\n",
       "  (1432, 1435),\n",
       "  (1435, 1436),\n",
       "  (1436, 1442),\n",
       "  (1443, 1447),\n",
       "  (1448, 1451),\n",
       "  (1452, 1457),\n",
       "  (1458, 1467),\n",
       "  (1468, 1471),\n",
       "  (1472, 1480),\n",
       "  (1480, 1481),\n",
       "  (1482, 1492),\n",
       "  (1493, 1496),\n",
       "  (1497, 1507),\n",
       "  (1507, 1508),\n",
       "  (1510, 1511),\n",
       "  (1511, 1512),\n",
       "  (1513, 1514),\n",
       "  (1514, 1519),\n",
       "  (1520, 1526),\n",
       "  (1526, 1529),\n",
       "  (1530, 1531),\n",
       "  (1532, 1537),\n",
       "  (1538, 1540),\n",
       "  (1541, 1543),\n",
       "  (1544, 1551),\n",
       "  (1552, 1554),\n",
       "  (1555, 1559),\n",
       "  (1560, 1565),\n",
       "  (1565, 1566),\n",
       "  (1569, 1570),\n",
       "  (1571, 1573),\n",
       "  (1574, 1581),\n",
       "  (1582, 1590),\n",
       "  (1591, 1594),\n",
       "  (1595, 1599),\n",
       "  (1600, 1612),\n",
       "  (1613, 1615),\n",
       "  (1616, 1625),\n",
       "  (1626, 1629),\n",
       "  (1630, 1637),\n",
       "  (1638, 1647),\n",
       "  (1648, 1650),\n",
       "  (1651, 1654),\n",
       "  (1655, 1663),\n",
       "  (1664, 1671),\n",
       "  (1671, 1672),\n",
       "  (1675, 1676),\n",
       "  (1677, 1682),\n",
       "  (1683, 1691),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 5),\n",
       "  (6, 10),\n",
       "  (11, 19),\n",
       "  (20, 29),\n",
       "  (30, 34),\n",
       "  (35, 36),\n",
       "  (37, 49),\n",
       "  (49, 50),\n",
       "  (0, 0),\n",
       "  (1146, 1148),\n",
       "  (1148, 1151),\n",
       "  (1151, 1152),\n",
       "  (1153, 1155),\n",
       "  (1156, 1168),\n",
       "  (1168, 1169),\n",
       "  (1170, 1174),\n",
       "  (1175, 1179),\n",
       "  (1180, 1182),\n",
       "  (1182, 1183),\n",
       "  (1183, 1186),\n",
       "  (1187, 1190),\n",
       "  (1190, 1193),\n",
       "  (1193, 1197),\n",
       "  (1198, 1204),\n",
       "  (1204, 1205),\n",
       "  (1206, 1210),\n",
       "  (1211, 1213),\n",
       "  (1214, 1218),\n",
       "  (1219, 1223),\n",
       "  (1224, 1227),\n",
       "  (1228, 1237),\n",
       "  (1237, 1238),\n",
       "  (1240, 1241),\n",
       "  (1241, 1242),\n",
       "  (1243, 1246),\n",
       "  (1246, 1249),\n",
       "  (1250, 1253),\n",
       "  (1254, 1259),\n",
       "  (1260, 1269),\n",
       "  (1270, 1273),\n",
       "  (1274, 1279),\n",
       "  (1280, 1284),\n",
       "  (1285, 1287),\n",
       "  (1288, 1289),\n",
       "  (1290, 1295),\n",
       "  (1295, 1296),\n",
       "  (1296, 1297),\n",
       "  (1298, 1306),\n",
       "  (1306, 1307),\n",
       "  (1310, 1311),\n",
       "  (1312, 1317),\n",
       "  (1318, 1323),\n",
       "  (1323, 1324),\n",
       "  (1324, 1326),\n",
       "  (1326, 1327),\n",
       "  (1327, 1330),\n",
       "  (1330, 1331),\n",
       "  (1331, 1334),\n",
       "  (1335, 1341),\n",
       "  (1342, 1344),\n",
       "  (1345, 1346),\n",
       "  (1347, 1352),\n",
       "  (1353, 1355),\n",
       "  (1356, 1360),\n",
       "  (1360, 1361),\n",
       "  (1364, 1365),\n",
       "  (1366, 1370),\n",
       "  (1371, 1372),\n",
       "  (1373, 1379),\n",
       "  (1380, 1385),\n",
       "  (1386, 1393),\n",
       "  (1394, 1395),\n",
       "  (1395, 1396),\n",
       "  (1396, 1397),\n",
       "  (1397, 1398),\n",
       "  (1398, 1399),\n",
       "  (1399, 1400),\n",
       "  (1400, 1401),\n",
       "  (1401, 1402),\n",
       "  (1402, 1403),\n",
       "  (1403, 1405),\n",
       "  (1405, 1407),\n",
       "  (1408, 1417),\n",
       "  (1417, 1418),\n",
       "  (1419, 1421),\n",
       "  (1422, 1426),\n",
       "  (1426, 1427),\n",
       "  (1430, 1431),\n",
       "  (1432, 1435),\n",
       "  (1435, 1436),\n",
       "  (1436, 1442),\n",
       "  (1443, 1447),\n",
       "  (1448, 1451),\n",
       "  (1452, 1457),\n",
       "  (1458, 1467),\n",
       "  (1468, 1471),\n",
       "  (1472, 1480),\n",
       "  (1480, 1481),\n",
       "  (1482, 1492),\n",
       "  (1493, 1496),\n",
       "  (1497, 1507),\n",
       "  (1507, 1508),\n",
       "  (1510, 1511),\n",
       "  (1511, 1512),\n",
       "  (1513, 1514),\n",
       "  (1514, 1519),\n",
       "  (1520, 1526),\n",
       "  (1526, 1529),\n",
       "  (1530, 1531),\n",
       "  (1532, 1537),\n",
       "  (1538, 1540),\n",
       "  (1541, 1543),\n",
       "  (1544, 1551),\n",
       "  (1552, 1554),\n",
       "  (1555, 1559),\n",
       "  (1560, 1565),\n",
       "  (1565, 1566),\n",
       "  (1569, 1570),\n",
       "  (1571, 1573),\n",
       "  (1574, 1581),\n",
       "  (1582, 1590),\n",
       "  (1591, 1594),\n",
       "  (1595, 1599),\n",
       "  (1600, 1612),\n",
       "  (1613, 1615),\n",
       "  (1616, 1625),\n",
       "  (1626, 1629),\n",
       "  (1630, 1637),\n",
       "  (1638, 1647),\n",
       "  (1648, 1650),\n",
       "  (1651, 1654),\n",
       "  (1655, 1663),\n",
       "  (1664, 1671),\n",
       "  (1671, 1672),\n",
       "  (1675, 1676),\n",
       "  (1677, 1682),\n",
       "  (1683, 1691),\n",
       "  (1691, 1692),\n",
       "  (1693, 1696),\n",
       "  (1697, 1704),\n",
       "  (1705, 1707),\n",
       "  (1708, 1720),\n",
       "  (1721, 1723),\n",
       "  (1724, 1732),\n",
       "  (1732, 1733),\n",
       "  (1736, 1737),\n",
       "  (1738, 1743),\n",
       "  (1744, 1749),\n",
       "  (1750, 1753),\n",
       "  (1754, 1756),\n",
       "  (1757, 1761),\n",
       "  (1762, 1775),\n",
       "  (1776, 1778),\n",
       "  (1779, 1782),\n",
       "  (1783, 1790),\n",
       "  (1791, 1794),\n",
       "  (1795, 1800),\n",
       "  (1801, 1812),\n",
       "  (1812, 1813),\n",
       "  (1815, 1816),\n",
       "  (1817, 1829),\n",
       "  (1830, 1832),\n",
       "  (1833, 1839),\n",
       "  (1840, 1842),\n",
       "  (1843, 1846),\n",
       "  (1847, 1852),\n",
       "  (1853, 1857),\n",
       "  (1858, 1865),\n",
       "  (1866, 1870),\n",
       "  (1871, 1879),\n",
       "  (1880, 1889),\n",
       "  (1890, 1891),\n",
       "  (1892, 1895),\n",
       "  (1895, 1896),\n",
       "  (1897, 1898),\n",
       "  (1898, 1899),\n",
       "  (1899, 1900),\n",
       "  (1900, 1902),\n",
       "  (1902, 1904),\n",
       "  (1905, 1908),\n",
       "  (1909, 1912),\n",
       "  (1912, 1915),\n",
       "  (1915, 1916),\n",
       "  (1916, 1919),\n",
       "  (1920, 1921),\n",
       "  (1922, 1926),\n",
       "  (1927, 1928),\n",
       "  (1929, 1932),\n",
       "  (1932, 1933),\n",
       "  (1933, 1937),\n",
       "  (1938, 1949),\n",
       "  (1950, 1957),\n",
       "  (1958, 1962),\n",
       "  (1962, 1963),\n",
       "  (1964, 1966),\n",
       "  (1966, 1967),\n",
       "  (1967, 1968),\n",
       "  (1969, 1984),\n",
       "  (1985, 1987),\n",
       "  (1988, 1993),\n",
       "  (1994, 1998),\n",
       "  (1999, 2005),\n",
       "  (2006, 2010),\n",
       "  (2011, 2014),\n",
       "  (2015, 2021),\n",
       "  (2022, 2029),\n",
       "  (2030, 2034),\n",
       "  (2035, 2038),\n",
       "  (2039, 2041),\n",
       "  (2041, 2048),\n",
       "  (2049, 2053),\n",
       "  (2054, 2057),\n",
       "  (2058, 2063),\n",
       "  (2063, 2064),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0)]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852409e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f30c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602cce45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59640858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6430e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a56664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f8dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e507946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23f5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e8937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286f165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a222881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a35ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
